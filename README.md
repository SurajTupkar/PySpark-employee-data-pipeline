Employee Data Processing Pipeline (PySpark)

ğŸ“Œ Project Overview

-> This project is an end-to-end PySpark data pipeline designed to process employee data in a structured, scalable, and production-ready manner.
-> The pipeline demonstrates core Data Engineering fundamentals, including:
   1) Schema enforcement
   2) Data cleaning & validation
   3) Business transformations
   4) Analytical aggregations
   5) Partitioned storage
   6) Centralized logging



ğŸ—ï¸ Architecture Flow

                                                Raw CSV Data
                                                    â†“
                                     Reader (Schema + Corrupt Handling)
                                                    â†“
                                     Cleaner (Nulls, Rename, Casting)
                                                    â†“
                                       Transformer (Business Logic)
                                                    â†“
                                    Writer (Partitioned Parquet Output)


ğŸ“‚ Project Folder Structure

```
PySpark/Employee Data Processing Pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ employees.csv
â”‚   â”‚
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ employees/
â”‚       â”œâ”€â”€ department_summary/
â”‚       â””â”€â”€ corrupt_records/
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ app.log
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ reader.py
â”‚   â”œâ”€â”€ cleaner.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â”œâ”€â”€ writer.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ venv/                 # ignored via .gitignore
â”œâ”€â”€ __pycache__/          # ignored via .gitignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

âš™ï¸ Technologies Used

-> Python 3.12
-> Apache Spark 3.5.1
-> PySpark
-> WSL (Ubuntu on Windows)
-> Git & GitHub
-> VS Code



ğŸ“¥ Input Data

employees.csv

Contains employee-level information such as:
employee_id
full_name
department
location
status
base_salary
bonus
joining_date
The pipeline handles:
Missing values
Incorrect schema
Corrupt records


ğŸ”„ Processing Logic

1. Reader (reader.py)
-> Enforces schema using StructType
-> Reads CSV in FAILFAST mode
-> Captures corrupt records using _corrupt_record

2. Cleaner (cleaner.py)
-> Separates valid and corrupt records
-> Renames columns for consistency
-> Handles null values
-> Performs data type casting
-> Converts joining_date to DateType

3. Transformer (transformer.py)
-> Calculates total_compensation
-> Filters Active employees only
-> Categorizes compensation (HIGH / NORMAL)
-> Department-level aggregations:
-> Average Salary
-> Total Bonus
-> Employee Count


4. Writer (writer.py)
-> Writes data in parquet format
-> Uses partitioning for optimized querying
-> Output paths:
                -> /employees
                -> /department_summary
                -> /corrupt_records


ğŸ“ Logging
-> Centralized logging using python logging.
-> Logs are written to "logs/app.log".
-> Logs are also visible on the terminal.
-> Each module logs its own execution steps


How to Run the Project

1. Activate Virtual Environment
source venv/bin/activate

2. Run the Pipeline
python src/main.py


ğŸ¯ Key Highlights

-> Modular, production-ready code structure
-> Schema enforcement & corrupt record handling
-> Partitioned Parquet output for performance
-> Clean separation of responsibilities
-> Interview-ready Data Engineering project


ğŸš€ Future Enhancements

-> Unit testing with PyTest
-> Configuration management via YAML
-> Spark submit integration
-> Cloud storage support (S3/ADLS)
-> Workflow orchestration (Airflow)


ğŸ‘¨â€ğŸ’» Author
Suraj Tupkar
Data Engineer
PYTHON | SQL | PySpark | AWS


